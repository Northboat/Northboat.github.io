<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>马尔可夫决策和强化学习 | Arkrypto</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_3129839_xft6cqs5gc.css">
    <noscript><meta http-equiv="refresh" content="0; url=https://www.youngkbt.cn/noscript/"><style>.theme-vdoing-content { display:none }</noscript>
    <meta name="description" content="My Wiki">
    <meta name="keywords" content="Arkrypto, ComputerScience, DevOps, Crypto">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.9427377d.css" as="style"><link rel="preload" href="/assets/js/app.c56dd4ae.js" as="script"><link rel="preload" href="/assets/js/3.182b3e92.js" as="script"><link rel="preload" href="/assets/js/39.e12c41d5.js" as="script"><link rel="preload" href="/assets/js/171.5e1a14ad.js" as="script"><link rel="prefetch" href="/assets/js/10.bb454b9b.js"><link rel="prefetch" href="/assets/js/100.f8745e9a.js"><link rel="prefetch" href="/assets/js/101.e29a09d3.js"><link rel="prefetch" href="/assets/js/102.5b5e61c6.js"><link rel="prefetch" href="/assets/js/103.27ba94c6.js"><link rel="prefetch" href="/assets/js/104.cffde887.js"><link rel="prefetch" href="/assets/js/105.96e0d066.js"><link rel="prefetch" href="/assets/js/106.9114dc37.js"><link rel="prefetch" href="/assets/js/107.49ec8ffe.js"><link rel="prefetch" href="/assets/js/108.e4532e66.js"><link rel="prefetch" href="/assets/js/109.ef5ef2f3.js"><link rel="prefetch" href="/assets/js/11.5180e68c.js"><link rel="prefetch" href="/assets/js/110.24d3f7c9.js"><link rel="prefetch" href="/assets/js/111.92348784.js"><link rel="prefetch" href="/assets/js/112.9c608873.js"><link rel="prefetch" href="/assets/js/113.21f7ffab.js"><link rel="prefetch" href="/assets/js/114.5e9d3a22.js"><link rel="prefetch" href="/assets/js/115.6b7ef1a2.js"><link rel="prefetch" href="/assets/js/116.0782db99.js"><link rel="prefetch" href="/assets/js/117.1bb13007.js"><link rel="prefetch" href="/assets/js/118.e7f745eb.js"><link rel="prefetch" href="/assets/js/119.a2e07422.js"><link rel="prefetch" href="/assets/js/12.34b70802.js"><link rel="prefetch" href="/assets/js/120.0e978d0b.js"><link rel="prefetch" href="/assets/js/121.4d65888e.js"><link rel="prefetch" href="/assets/js/122.1d869d7e.js"><link rel="prefetch" href="/assets/js/123.c02b05f5.js"><link rel="prefetch" href="/assets/js/124.05956a9f.js"><link rel="prefetch" href="/assets/js/125.f35baccb.js"><link rel="prefetch" href="/assets/js/126.56191258.js"><link rel="prefetch" href="/assets/js/127.28ff6cc0.js"><link rel="prefetch" href="/assets/js/128.69282104.js"><link rel="prefetch" href="/assets/js/129.adf5219c.js"><link rel="prefetch" href="/assets/js/13.669c25c5.js"><link rel="prefetch" href="/assets/js/130.182090df.js"><link rel="prefetch" href="/assets/js/131.df2f2a50.js"><link rel="prefetch" href="/assets/js/132.71094567.js"><link rel="prefetch" href="/assets/js/133.50ea9fd0.js"><link rel="prefetch" href="/assets/js/134.85f7a612.js"><link rel="prefetch" href="/assets/js/135.8474f459.js"><link rel="prefetch" href="/assets/js/136.f63613f2.js"><link rel="prefetch" href="/assets/js/137.c3886c7e.js"><link rel="prefetch" href="/assets/js/138.c7f46931.js"><link rel="prefetch" href="/assets/js/139.9f7a5c9c.js"><link rel="prefetch" href="/assets/js/14.9f77c8db.js"><link rel="prefetch" href="/assets/js/140.5af52f55.js"><link rel="prefetch" href="/assets/js/141.e2e64c45.js"><link rel="prefetch" href="/assets/js/142.257adc59.js"><link rel="prefetch" href="/assets/js/143.da4d1b36.js"><link rel="prefetch" href="/assets/js/144.437016e0.js"><link rel="prefetch" href="/assets/js/145.e7062e4e.js"><link rel="prefetch" href="/assets/js/146.ddc4c4ed.js"><link rel="prefetch" href="/assets/js/147.62980f25.js"><link rel="prefetch" href="/assets/js/148.7f6b7e48.js"><link rel="prefetch" href="/assets/js/149.22d80d7f.js"><link rel="prefetch" href="/assets/js/15.8f9fa1da.js"><link rel="prefetch" href="/assets/js/150.c2b4e366.js"><link rel="prefetch" href="/assets/js/151.b89a4659.js"><link rel="prefetch" href="/assets/js/152.31bdd939.js"><link rel="prefetch" href="/assets/js/153.f6f5eda9.js"><link rel="prefetch" href="/assets/js/154.a070c3b6.js"><link rel="prefetch" href="/assets/js/155.66c31154.js"><link rel="prefetch" href="/assets/js/156.b7ee18cb.js"><link rel="prefetch" href="/assets/js/157.28e54600.js"><link rel="prefetch" href="/assets/js/158.a226a0ea.js"><link rel="prefetch" href="/assets/js/159.5f58a02c.js"><link rel="prefetch" href="/assets/js/16.9fa6e1dc.js"><link rel="prefetch" href="/assets/js/160.63aae841.js"><link rel="prefetch" href="/assets/js/161.630149f2.js"><link rel="prefetch" href="/assets/js/162.7d8da323.js"><link rel="prefetch" href="/assets/js/163.2bd0e288.js"><link rel="prefetch" href="/assets/js/164.c413e1bc.js"><link rel="prefetch" href="/assets/js/165.0c9b04c8.js"><link rel="prefetch" href="/assets/js/166.b7edf6a8.js"><link rel="prefetch" href="/assets/js/167.5459122f.js"><link rel="prefetch" href="/assets/js/168.3ba69ff5.js"><link rel="prefetch" href="/assets/js/169.af583f47.js"><link rel="prefetch" href="/assets/js/17.a4f20625.js"><link rel="prefetch" href="/assets/js/170.370d459c.js"><link rel="prefetch" href="/assets/js/172.dcf7ab41.js"><link rel="prefetch" href="/assets/js/173.68a1a35d.js"><link rel="prefetch" href="/assets/js/174.94ee4f75.js"><link rel="prefetch" href="/assets/js/175.33e81ea7.js"><link rel="prefetch" href="/assets/js/176.72fb33b9.js"><link rel="prefetch" href="/assets/js/177.81c826e5.js"><link rel="prefetch" href="/assets/js/178.08d7506e.js"><link rel="prefetch" href="/assets/js/179.85b37c39.js"><link rel="prefetch" href="/assets/js/18.3063521b.js"><link rel="prefetch" href="/assets/js/180.556bace2.js"><link rel="prefetch" href="/assets/js/181.92c8b77f.js"><link rel="prefetch" href="/assets/js/182.a0e7a5ed.js"><link rel="prefetch" href="/assets/js/183.b27514b4.js"><link rel="prefetch" href="/assets/js/184.a9d51057.js"><link rel="prefetch" href="/assets/js/185.b4e0c103.js"><link rel="prefetch" href="/assets/js/186.50c99037.js"><link rel="prefetch" href="/assets/js/187.945428d9.js"><link rel="prefetch" href="/assets/js/188.b54e39d9.js"><link rel="prefetch" href="/assets/js/189.fd85d533.js"><link rel="prefetch" href="/assets/js/19.5b6e93a0.js"><link rel="prefetch" href="/assets/js/190.f43c7671.js"><link rel="prefetch" href="/assets/js/191.503acb57.js"><link rel="prefetch" href="/assets/js/192.53a4ba5b.js"><link rel="prefetch" href="/assets/js/193.26171f3e.js"><link rel="prefetch" href="/assets/js/194.3b96d65f.js"><link rel="prefetch" href="/assets/js/195.7fd5e272.js"><link rel="prefetch" href="/assets/js/2.464641c8.js"><link rel="prefetch" href="/assets/js/20.650edd8b.js"><link rel="prefetch" href="/assets/js/21.2150dc8c.js"><link rel="prefetch" href="/assets/js/22.23056eed.js"><link rel="prefetch" href="/assets/js/23.a1331097.js"><link rel="prefetch" href="/assets/js/24.5bcee318.js"><link rel="prefetch" href="/assets/js/25.663f8b4f.js"><link rel="prefetch" href="/assets/js/26.d565a283.js"><link rel="prefetch" href="/assets/js/27.702f4907.js"><link rel="prefetch" href="/assets/js/28.4e909f85.js"><link rel="prefetch" href="/assets/js/29.ee0ef100.js"><link rel="prefetch" href="/assets/js/30.ad084af2.js"><link rel="prefetch" href="/assets/js/31.2160b6af.js"><link rel="prefetch" href="/assets/js/32.6556397c.js"><link rel="prefetch" href="/assets/js/33.a454a1c2.js"><link rel="prefetch" href="/assets/js/34.f6a8962f.js"><link rel="prefetch" href="/assets/js/35.ef0660c3.js"><link rel="prefetch" href="/assets/js/36.0313b6e1.js"><link rel="prefetch" href="/assets/js/37.f73ff21e.js"><link rel="prefetch" href="/assets/js/38.c87b979a.js"><link rel="prefetch" href="/assets/js/4.eb91d11d.js"><link rel="prefetch" href="/assets/js/40.b6458d03.js"><link rel="prefetch" href="/assets/js/41.cd79cad8.js"><link rel="prefetch" href="/assets/js/42.ac39958a.js"><link rel="prefetch" href="/assets/js/43.f03d304d.js"><link rel="prefetch" href="/assets/js/44.e2dc0347.js"><link rel="prefetch" href="/assets/js/45.dd864202.js"><link rel="prefetch" href="/assets/js/46.433e1f74.js"><link rel="prefetch" href="/assets/js/47.ec87c684.js"><link rel="prefetch" href="/assets/js/48.20cec508.js"><link rel="prefetch" href="/assets/js/49.1ec38123.js"><link rel="prefetch" href="/assets/js/5.e2c0cd36.js"><link rel="prefetch" href="/assets/js/50.4686ec96.js"><link rel="prefetch" href="/assets/js/51.dd1e0034.js"><link rel="prefetch" href="/assets/js/52.200541eb.js"><link rel="prefetch" href="/assets/js/53.f9b461ad.js"><link rel="prefetch" href="/assets/js/54.c419244d.js"><link rel="prefetch" href="/assets/js/55.6ac819b7.js"><link rel="prefetch" href="/assets/js/56.9cfe9a44.js"><link rel="prefetch" href="/assets/js/57.8f3fed00.js"><link rel="prefetch" href="/assets/js/58.51c2f255.js"><link rel="prefetch" href="/assets/js/59.d3bf4662.js"><link rel="prefetch" href="/assets/js/6.ba514a35.js"><link rel="prefetch" href="/assets/js/60.3a5be447.js"><link rel="prefetch" href="/assets/js/61.39380471.js"><link rel="prefetch" href="/assets/js/62.b95b9cbd.js"><link rel="prefetch" href="/assets/js/63.4e5fefa8.js"><link rel="prefetch" href="/assets/js/64.3f631a3d.js"><link rel="prefetch" href="/assets/js/65.37ddde47.js"><link rel="prefetch" href="/assets/js/66.7ac74c38.js"><link rel="prefetch" href="/assets/js/67.0775d05a.js"><link rel="prefetch" href="/assets/js/68.51750578.js"><link rel="prefetch" href="/assets/js/69.8ae406f4.js"><link rel="prefetch" href="/assets/js/7.50540523.js"><link rel="prefetch" href="/assets/js/70.a117de2b.js"><link rel="prefetch" href="/assets/js/71.90bf25d5.js"><link rel="prefetch" href="/assets/js/72.70d3b094.js"><link rel="prefetch" href="/assets/js/73.ec153b36.js"><link rel="prefetch" href="/assets/js/74.a96b5c3b.js"><link rel="prefetch" href="/assets/js/75.f1935957.js"><link rel="prefetch" href="/assets/js/76.c936d56c.js"><link rel="prefetch" href="/assets/js/77.9edf3648.js"><link rel="prefetch" href="/assets/js/78.18a19b1b.js"><link rel="prefetch" href="/assets/js/79.aea0d817.js"><link rel="prefetch" href="/assets/js/8.8860098f.js"><link rel="prefetch" href="/assets/js/80.ef721569.js"><link rel="prefetch" href="/assets/js/81.1de26ad4.js"><link rel="prefetch" href="/assets/js/82.aacdad7f.js"><link rel="prefetch" href="/assets/js/83.3d4618ac.js"><link rel="prefetch" href="/assets/js/84.14714c54.js"><link rel="prefetch" href="/assets/js/85.19d6fcbf.js"><link rel="prefetch" href="/assets/js/86.3ff48f97.js"><link rel="prefetch" href="/assets/js/87.2efb7897.js"><link rel="prefetch" href="/assets/js/88.031fed2d.js"><link rel="prefetch" href="/assets/js/89.78c928f7.js"><link rel="prefetch" href="/assets/js/9.93305e98.js"><link rel="prefetch" href="/assets/js/90.5dd8b0f1.js"><link rel="prefetch" href="/assets/js/91.04314f0a.js"><link rel="prefetch" href="/assets/js/92.64448bd9.js"><link rel="prefetch" href="/assets/js/93.d39ddcdb.js"><link rel="prefetch" href="/assets/js/94.8f213249.js"><link rel="prefetch" href="/assets/js/95.5aad521d.js"><link rel="prefetch" href="/assets/js/96.62517ca5.js"><link rel="prefetch" href="/assets/js/97.ad78e522.js"><link rel="prefetch" href="/assets/js/98.49915ff0.js"><link rel="prefetch" href="/assets/js/99.d45e8ea3.js">
    <link rel="stylesheet" href="/assets/css/0.styles.9427377d.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/logo.png" alt="Arkrypto" class="logo"> <span class="site-name can-hide">Arkrypto</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/cs/" class="nav-link">计算机科学与技术</a></div><div class="nav-item"><a href="/dev/" class="nav-link">开发与运维</a></div><div class="nav-item"><a href="/sec/" class="nav-link">网络与信息安全</a></div><div class="nav-item"><a href="/mine/" class="nav-link">我的</a></div><div class="nav-item"><a href="/archives/" class="nav-link">归档</a></div> <a href="https://github.com/Arkrypto" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/avatar.gif"> <div class="blogger-info"><h3>Arkrypto</h3> <span>就在坚冰还盖着北海的时候，我看到了怒放的梅花</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/cs/" class="nav-link">计算机科学与技术</a></div><div class="nav-item"><a href="/dev/" class="nav-link">开发与运维</a></div><div class="nav-item"><a href="/sec/" class="nav-link">网络与信息安全</a></div><div class="nav-item"><a href="/mine/" class="nav-link">我的</a></div><div class="nav-item"><a href="/archives/" class="nav-link">归档</a></div> <a href="https://github.com/Arkrypto" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>密码工程</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>数学</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>人工智能</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>人工智能导论</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/288d2a/" class="sidebar-link">搜索、约束满足和博弈</a></li><li><a href="/pages/17b001/" aria-current="page" class="active sidebar-link">马尔可夫决策和强化学习</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/17b001/#mdps" class="sidebar-link">MDPs</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/17b001/#what-is-mdps" class="sidebar-link">What is MDPs</a></li><li class="sidebar-sub-header level3"><a href="/pages/17b001/#solving-mdps" class="sidebar-link">Solving MDPs</a></li><li class="sidebar-sub-header level3"><a href="/pages/17b001/#summary" class="sidebar-link">Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/17b001/#rl" class="sidebar-link">RL</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/17b001/#model-based-learning" class="sidebar-link">Model-Based Learning</a></li><li class="sidebar-sub-header level3"><a href="/pages/17b001/#model-free-learning" class="sidebar-link">Model-Free Learning</a></li></ul></li></ul></li><li><a href="/pages/f0dd9d/" class="sidebar-link">不确定知识和概率推理</a></li><li><a href="/pages/54be26/" class="sidebar-link">隐马尔科夫和机器学习</a></li><li><a href="/pages/99e92c/" class="sidebar-link">经典人工智能算法实现</a></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>机器学习导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>深度学习框架</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06970110><div class="articleInfo" data-v-06970110><ul class="breadcrumbs" data-v-06970110><li data-v-06970110><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06970110></a></li> <li data-v-06970110><a href="/sec/#网络与信息安全" data-v-06970110>网络与信息安全</a></li><li data-v-06970110><a href="/sec/#人工智能" data-v-06970110>人工智能</a></li><li data-v-06970110><a href="/sec/#人工智能导论" data-v-06970110>人工智能导论</a></li></ul> <div class="info" data-v-06970110><div title="作者" class="author iconfont icon-touxiang" data-v-06970110><a href="https://github.com/Arkrypto" target="_blank" title="作者" class="beLink" data-v-06970110>Arkrypto</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06970110><a href="javascript:;" data-v-06970110>2022-4-24</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">马尔可夫决策和强化学习<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h2 id="mdps"><a href="#mdps" class="header-anchor">#</a> MDPs</h2> <blockquote><p>Markov Decision Processes</p> <p>马尔可夫决定过程</p></blockquote> <h3 id="what-is-mdps"><a href="#what-is-mdps" class="header-anchor">#</a> What is MDPs</h3> <p>进程：对搜索的概括</p> <p>计算可能的结果</p> <p>在<code>GridWorld</code>中，你决定向北走（因为这是最佳策略），但可能会执行失败（撞墙）</p> <p>MDP：Reward ——&gt; 结果</p> <ul><li>happy reward</li> <li>bad reward</li></ul> <p>目标很松散 ——&gt;为了最大化奖励的总和</p> <p>An MDP is defined by</p> <ul><li><p>a set of states s</p></li> <li><p>a set of actions a</p></li> <li><p>a transition function T(s, a, s')</p> <ul><li><p>Probability that a from s leads to s', called P(s'| s, a)</p> <p>在s状态执行a行为到达s'的代价</p></li> <li><p>Also called the model or the dynamics</p> <p>不同于搜索，这个后继函数有很多个，如在每个地点都可以向东南西北移动</p></li></ul></li> <li><p>a reward function R(s, a, s')</p> <ul><li><p>sometimes just R(s) or R(s')</p> <p>奖惩制度，有时只取决起点或终点</p></li></ul></li> <li><p>a start state</p></li> <li><p>maybe a terminal state</p></li></ul> <p>MDPs是不确定性搜索问题</p> <ul><li>强化学习的基础</li></ul> <p>expectimax（最大期望算法）算法可以MDP问题</p> <p>action outcomes depend on</p> <ul><li>未来要到达的状态</li> <li>你要执行的行动</li></ul> <p>MDPs适合嘈杂的世界</p> <p><strong>Grid World</strong></p> <p>Policy：策略</p> <ul><li>通过状态告诉你动作的功能</li> <li>如在地图上每个点标好你该往哪个方向走</li></ul> <p>optimal policy：最优策略</p> <ul><li>或许存在很多等效策略</li></ul> <p>competition</p> <ul><li>移动奖励（负0.1）是那么微不足道而不值得冒险去坑附近</li> <li>宁愿什么都不做，也不愿犯错</li></ul> <p>当移动代价变得更大，策略将更倾向与冒险在坑附近</p> <p>当更更大时，甚至有可能直接跳坑而避免移动花销</p> <p><strong>Racing</strong></p> <p>states：</p> <ul><li>cool</li> <li>warm</li> <li>overheated：risk the danger of breaking</li></ul> <p>跟据当前的温度决定是加速还是减速</p> <p><strong>Racing Search Tree</strong></p> <blockquote><p>tool：epectimax search</p></blockquote> <p>actions：</p> <ul><li>slower</li> <li>faster</li></ul> <p>state：</p> <ul><li>warm</li> <li>cool</li> <li>over heated</li></ul> <p>这棵树是无限的</p> <ul><li>Q state：选择了但还没行动的过度状态</li></ul> <p><strong>Utilities of Sequences</strong></p> <p>实用程序的选择顺序</p> <ul><li>more or less</li> <li>now or later</li></ul> <p>隐含的权衡</p> <p><strong>discount</strong></p> <p>对奖励的贬值，对晚来的价值施以惩罚，如每走一步，未得到的价值便腐朽0.8，0.8便是折扣</p> <p>当折扣越大，即<code>λ</code>越小，agent将变得越贪婪，越在意眼前的价值，而不是以后获得更大的利益</p> <p><strong>Preferences</strong></p> <p>假设偏好是固定的</p> <p>two ways to define utilities</p> <ul><li>additive utility</li> <li>discounted utility</li></ul> <p>如何处理无限的问题</p> <ul><li>Finite horizen：similar to depth-limited search，即限定树的深度</li> <li>Discounting：价值总是贬值，将无限接近于0</li> <li>Absorbing state：使用一系列终止状态，即</li></ul> <p>Markov decision processes：</p> <ul><li>状态集</li> <li>初始状态</li> <li>行为集</li> <li>过渡函数：提供的是概率</li> <li>奖惩机制</li></ul> <p>它的输出是每个state上对应的action，他实际上并没有真正在试错，而是去给每个状态分配最佳的行动，这就是MDP</p> <h3 id="solving-mdps"><a href="#solving-mdps" class="header-anchor">#</a> Solving MDPs</h3> <p>Quantities：</p> <ul><li>Policy：map of states of actions</li> <li>Utility：sum of discounted reward</li> <li>Values：expected future utility from a state（max node）</li> <li>Q Values：expected future from a q-state（chance node）</li></ul> <p>Optimal Quantities</p> <ul><li>V(s)*：状态的期望值（或许是平均值）</li> <li>Q*(s, a)：在状态s执行动作a后起的最佳作用</li> <li>P*(s)：当前状态的最佳策略（算法产出）</li></ul> <p>expectimax search可以解决这一问题，估算价值，选出最大价值，赋值</p> <p>考虑一下其他的算法</p> <ul><li><p><code>V*(s) = maxQ*(s, a)</code></p> <p>虽然Q*(s, a)还不知道怎么算</p></li> <li><p><code>Q*(s, a) = avg(sum(R(s, a, s') + λV*(s'))</code></p> <p>这是一个递归的定义，因为你并不知道V*(s')直到搜索到终点</p></li></ul> <p>此之谓贝尔曼方程：Bellman Equations</p> <ul><li>take correct first action</li> <li>kepp being optimal</li></ul> <p>回顾一下Racing Search Tree</p> <p>他是无限的，并且只有三种状态，如果用expectimax search，将会有指数级的重复工作（子树）</p> <h4 id="value-iteration"><a href="#value-iteration" class="header-anchor">#</a> Value Iteration</h4> <p>价值迭代算法</p> <ul><li>from the bottom（deep enough）, recur the top</li> <li><code>V*(s) = maxΣT(s,a,s')[R(s,a,s') + λV*(s')]</code></li></ul> <p>利用贝尔曼方程确实可以搜索到底部并且递归回顶部，在这个递归过程中，各节点的值是不断更新的，且更加准确，直到保持稳定，即递归完毕</p> <ul><li>这个收敛的过程称作<code>bellman update</code></li></ul> <p><strong>Computing Time-Limited Values</strong></p> <p>对于一颗无限树，采用时间限制其递归深度，令V*(s)尽可能准确</p> <p>因为条件有限，我们无法完整进行贝尔曼算法，即只能尽可能的接近V*(s)</p> <ul><li><code>Vk(s) = avg(sum(R(s, a, s') + λVk(s')))</code></li></ul> <p>其中<code>Vk(s)、Vk(s')</code>都取其均值</p> <ul><li>take average</li> <li>像一个单层的expectimax搜索，但不同的是，他会由于递归深度的增加不断调整Vk值</li></ul> <p><strong>Convergence</strong></p> <p>VK compute</p> <p>一个k层树和一个k+1层树</p> <p>由于搜索深度增加，对于未来某节点的折扣也增加，也就是说越往后对总值的影响应是越小，细微调整</p> <p>当discount&gt;=1，没有趋同保证</p> <h4 id="policy-evaluation"><a href="#policy-evaluation" class="header-anchor">#</a> Policy Evaluation</h4> <p>策略评估方法</p> <h5 id="fixed-policies"><a href="#fixed-policies" class="header-anchor">#</a> fixed Policies</h5> <p>固定的策略</p> <ul><li>do the optimal action</li> <li>do what Pi says</li> <li>easier than the optimal</li></ul> <p>假设你的固定策略选出的后继节点是最佳的</p> <ul><li><code>VΠ(s) = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></li></ul> <p>固定策略例如：一直向右走；一直向前走</p> <p>列举所有策略，评估所有策略，选择得分最高的策略</p> <h5 id="policy-evaluation-2"><a href="#policy-evaluation-2" class="header-anchor">#</a> policy evaluation</h5> <p>输入一个策略，执行策略，得到该策略的值向量</p> <p><code>VΠ = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></p> <h5 id="policy-extraction"><a href="#policy-extraction" class="header-anchor">#</a> Policy Extraction</h5> <p>即使当找到了相邻的最佳值，仍然要做一次expectimax去找到导致这个最佳值的行动</p> <ul><li>从值中找出行动，以更新策略</li></ul> <p>价值驱动决策</p> <p>Computing Actions from Q-Values</p> <h4 id="policy-iteration"><a href="#policy-iteration" class="header-anchor">#</a> Policy Iteration</h4> <p>价值迭代的问题</p> <ul><li>每次迭代将会耗费<code>O(s^2*A)</code>，这很慢</li> <li>每个状态的最大值很少改变，这意味着做了很多低效工作</li></ul> <p>正确策略下的无用选择 ——&gt; 错误的策略试错</p> <p>我们采用策略迭代</p> <ul><li>首先选择一些策略，并执行他们，估算状态价值</li> <li>改善你的策略，再次考虑之前的行动，重复估值</li> <li>直到策略收敛</li></ul> <p>可以证明它是最佳且收敛的，并且在很多情况比价值迭代收敛得更快</p> <p>VΠ是由当前策略得到的当前“最佳值”</p> <p><code>VΠ = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></p> <p>根据这个当前最佳值，更新上一步的策略，比如我上一步策略原来是往北走，但这个最佳值得往东走，那么我更新上一步的策略为向东走</p> <ul><li>MDPs本质上便是找到每步的最佳策略，值迭代同时考虑策略和价值，在每步做出最佳选择；策略迭代通过值去找到更优的策略</li> <li>二者都是迭代，从叶子回溯到顶部</li></ul> <p>通常根据最后值的变化来确定是否已经收敛</p> <h3 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h3> <ul><li>compute optimal values：both can</li> <li>compute values for partivular policy：policy evaluation（策略评估）</li> <li>turn your values into policy：use policy extraction（策略抽取）</li></ul> <p>通常Policy Iteration是policy evaluation和policy improvement交替执行直到收敛</p> <p>Value Iteration是寻找Optimal value function和执行一次policy extraction</p> <ul><li>均属于动态规划算法</li></ul> <p><strong>Double-Bandit MDP</strong></p> <p>两台老虎机，一台（blue）拉一次给一块钱；另一台（red）拉一次给0元或2元。共拉一百次</p> <p>更优的策略？</p> <ul><li><code>red one</code>获得2元的概率为0.75</li></ul> <p>平均上</p> <ul><li>blue：100元</li> <li>red：150元</li></ul> <p>当获得2元的概率未知，尝试red one去获得信息</p> <p>core of reinfocement Learning：exploraton</p> <p>只能探索才能获取更多信息</p> <p>pay for the infomation and get return</p> <p>甚至不需要MDP算法，只需要不断探索和基本的数学直觉，试出概率</p> <h2 id="rl"><a href="#rl" class="header-anchor">#</a> RL</h2> <blockquote><p>Reinforcement learning：强化学习</p> <p>It's about how to learn behaviors</p></blockquote> <ul><li>Agent —actions—&gt; Environment</li> <li>Environment —state/reward—&gt; Agent</li></ul> <p>Agent和Environment都是动态变化的</p> <p>Basic idea：</p> <ul><li>agent接收奖惩反馈</li> <li>奖惩函数决定agent的效用</li> <li>为了最大化奖励，必须去学习最优行动</li> <li>所有的学习基于观察样例后的结果</li></ul> <p>learning rather than plan</p> <p>Examples：</p> <ul><li>Robot dog learning to Walk</li> <li>Snake rebot sidewingding（爬墙）</li></ul> <p>因为真实世界的规则并不是确定的，难以建模，这时让程序根据概率学习正确的行为显得更加高效</p> <ul><li><p>Toddler Robot（幼儿机器人）</p> <p>know how to stand after fall down</p></li></ul> <p>机器学习的最开始，他是不知道怎么做的，只是来回摆动，因为他不知道怎么获取奖励，于是开始瞎几把试，当偶然获取奖励后，他将根据奖惩制度完善自己的行动策略，从而行动得更加高效</p> <p>Still assume a Markov decision process</p> <ul><li><p>a set of states</p></li> <li><p>a set of actions</p></li> <li><p>a model T(s,a,s')</p> <p>原为 a successor function T(s,a,s')</p></li> <li><p>a reward function R(s,a,s')</p></li></ul> <p>Still looking for a policy</p> <p>The defference：We don't know T or R</p> <ul><li><p>不知道哪个状态是好的或哪个动作是好的</p> <p>就像那个老虎机不知道掉落概率</p></li> <li><p>必须真正去行动和访问状态去学习，去获取必要信息</p></li></ul> <p>Offline（MDPs） vs. Online（RL）</p> <ul><li>Offline Solution</li> <li>Online Learning</li></ul> <h3 id="model-based-learning"><a href="#model-based-learning" class="header-anchor">#</a> Model-Based Learning</h3> <p>Basic idea：</p> <ul><li>learn an approximate model based on experiences</li> <li>solve for values as if the learned model were correct</li></ul> <p>现根据经验构建模型，再使用问题求解方法去计算当前模型</p> <p>就像一个CSP问题我们不知道联系，得先建立相邻状态联系</p> <p>step1：learn empirical MDP model</p> <ul><li>为每个状态和动作做产出（outcomes）统计</li> <li>常态化评估函数T(s,a,s')</li> <li>每当经历<code>s—a—&gt;s'</code>时计算回报函数R(s,a,s')</li></ul> <p>step2：solve the learned MDP（近似的MDP问题）</p> <ul><li>use value iteration</li> <li>use policy iteration</li> <li>......</li></ul> <p>T和R是未知的，但状态空间和行为空间被分配了，要做的就是收集更多数据，动态改善你的模型，估计T和R函数</p> <p>where the reward function come from</p> <ul><li>depend on the human designer</li></ul> <p>how to calculate T function</p> <ul><li>in a simple example, may just looking at the frequencies（频率）</li></ul> <p>计算概率权值：E（概率x值）</p> <ul><li><p>Known P(A)：E(A) = ΣP(a)*a</p></li> <li><p>Unknown P(A)</p> <ul><li><p>Model Based：E(A) = avg(sum(P(a)*a))</p> <p>以某种策略重新计算概率</p></li> <li><p>Model free：E(A) = (1/N)*sum(a)</p> <p>我们认为各种可能概率是相等的，因为尚未总结出规律</p></li> <li><p>二者区别在于是否按概率加权计算均值</p></li></ul></li></ul> <h3 id="model-free-learning"><a href="#model-free-learning" class="header-anchor">#</a> Model-Free Learning</h3> <h4 id="value-learning"><a href="#value-learning" class="header-anchor">#</a> Value Learning</h4> <blockquote><p>Passive Reinforcement Learning</p> <p>我们不担心如何在世界模型中行动，只是观察行动并视图估计此代理的状态值</p></blockquote> <p>Simplified task：policy evaluation</p> <ul><li>input：a fixed policy（遵循某一策略）</li> <li>don't know T(s,a,s')</li> <li>don't know R(s,a,s')</li> <li>goal：learn the state values</li></ul> <p>Direct &amp; Indirect Evaluation</p> <blockquote><p>直接估值和间接估值</p></blockquote> <p>直接估值平均观察到的样本值，直接问这一步会有多少<code>reward</code>，仅仅依据实验出的结果的各状态值</p> <p>如直接对于个节点的可能取值求均值作为其状态值，如对C节点使用四次策略</p> <ul><li><p>C向D -1，D退出+10</p> <p>C向D -1，D退出+10</p> <p>C向D -1，D退出+10</p> <p>C向A -1，A退出-10</p> <p>那么取均值则为<code>(9+9+9-11)/4=4</code></p></li></ul> <p>不需要对T/R做任何事，求均值就行了，只关注值；这不能达到超精确，但随着数据增加总会愈加接近</p> <p>要做的事很明确：</p> <ul><li>选择一个节点</li> <li>多次使用策略进行扩展</li> <li>对扩展结果进行分析取均</li> <li>对该节点赋值得到<code>V(s)</code></li> <li>更新值和策略</li></ul> <p>这一过程始终没用到T/R函数</p> <ul><li><code>VΠ(s) &lt;-- (1/n)Σsample(i)</code></li></ul> <p>注意这里所有的<code>V(s')</code>都应乘上一个<code>λ(&lt;=1)</code>作为时间惩罚（贬值）</p> <p>Temporal difference learning：</p> <ul><li><code>sample = R(s,Π(s), s') + λV(s')</code></li> <li><code>VΠ(s) &lt;-- (1-a)VΠ(s) + (a)sample</code></li></ul> <p>以上为更新已走过节点的方法</p> <p>每次获得新的sample，都对刚走过的状态<code>s</code>进行更新，以接近精确值</p> <p>在这一过程中，我们从未建立世界模型，即T/R函数，只是根据样例值不断更新状态值，随着时间的推移，将得到精确值</p> <p>优化求均值的方法，让越接近的经历比以前的经历更重要，因为我们后来计算的结果总是更加准确</p> <ul><li><p><code>xn = (xn + (1-a)*xn-1 + (1-a)^2*xn-2+...) / 1+(1-a)+(1-a)^2+...</code></p> <p>xn为第n个样例</p></li> <li><p>这里的a为学习率，应用于迭代方程中</p></li></ul> <p>由于我们从未构建模型，也没有T/R函数，根本无从进行策略迭代</p> <p>为什么不学习<code>Q-Value</code>而是<code>V-Value</code>？</p> <p>没有理由，他不仅同样能实现更新Value，而且可以用于策略更新，属于积极的学习</p> <h4 id="q-learning"><a href="#q-learning" class="header-anchor">#</a> Q-Learning</h4> <blockquote><p>Active Reinforcement Learning</p> <p>担心数据从何处收集，担心采取行动</p></blockquote> <p>also</p> <ul><li>don't know the transitions T</li> <li>don't know the reward R</li> <li>choose the actions now（当前做的）</li> <li>goal：learn the optimal policy/values</li></ul> <p>不同于MDPs，这不是离线测试（毕竟不知道T/R，无法进行推测），而是真切地采取行动</p> <p>iteration</p> <ul><li>从一个确定状态值开始</li> <li>计算该状态值下一层每个状态的Q-Value和Value</li> <li>通过下一层的Q-Value/Value更新该层的Q-Value/Value</li> <li>迭代这一过程，更新所有Q-Value/Value</li></ul> <p>Value Iteration</p> <ul><li><code>Vk+1(s) &lt;-- maxΣT(s,a,s')[R(s,a,s') + λVk(s')]</code></li></ul> <p>Q-Value Iteration</p> <ul><li><code>Qk+1(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxQk(s',a')]</code></li></ul> <p>在这里使用的样例和更新策略</p> <ul><li><p><code>sample = R(s,a,s') + λmaxQk(s',a')</code></p></li> <li><p><code>Q(s,a) &lt;-- (1-a)Q(s,a) + (a)sample</code></p> <p>这个常量a称为学习率</p></li></ul> <p>举例：crawler bot（爬虫机器人）</p> <p>Q-Learning is called off-policy learning</p> <p>Caveats（警告）</p> <ul><li>have to explore enough</li> <li>have to eventually make the learning rate small enough（收敛）</li> <li>...but not decrease it too quickly</li> <li>it doesn't matter how you select actions</li></ul> <table><thead><tr><th>Problem</th> <th>Goal</th> <th>Technique</th></tr></thead> <tbody><tr><td>Known MDP</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>Value/Policy iteration; Policy evaluation</td></tr> <tr><td>Unknown MDP: Model-Based</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>VI/PI on approximate MDP; PE on approximate MDP</td></tr> <tr><td>Unknown MDP: Model-Free</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>Q-learning; Value learning</td></tr></tbody></table> <p>均使用贝尔曼方程进行递归计算</p> <p>Exploration（探索）vs. exploitation（开发）</p> <p><strong>Epsilon Greedy</strong></p> <p>Exploration function</p> <ul><li><p>探索未知节点，收集更多经验：random actions（ε epsilon-greedy）</p> <p>当ε越大，随机度越高，当为0，策略确定</p></li> <li><p>探索方程将根据一个节点的“经验”，如访问过多少次，来给予相应的奖励（访问越多，奖励越低）</p></li> <li><p><code>f(u,n) = u + k/n</code>（基数+奖励/访问次数）</p></li> <li><p>这样能有效腐烂一些无用的节点（越多访问奖励越少）</p></li></ul> <p>Q-Update：加入探索方程</p> <ul><li><p>Regular Q-Update:</p> <p><code>Q(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxQ(s',a')]</code></p></li> <li><p>Modified Q-Update:</p> <p><code>Q(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxf(Q(s',a'),N(s',a'))]</code></p></li></ul> <p>Regret</p> <h4 id="approximate-q-learning"><a href="#approximate-q-learning" class="header-anchor">#</a> Approximate Q-Learning</h4> <p>在实际问题中，状态数、动作会很多很多，很难在Q-Table中去储存每一个Q-Value，这个时候只能做估计</p> <p>w为权重，f为特征值（features）</p> <ul><li><code>V(s) = w1*f1(s)+w2*f2(s)+...+wn*fn(s)</code></li> <li><code>Q(s,a) = w1*f1(s,a)+w2*f2(s,a)+...+wn*fn(s,a)</code></li></ul> <p>你的Q值将是很多经验的加权和，如f1为跳楼的特征值，f2为纵火的特征值，Q将这些情况的经验汇总以某些权重组合</p> <p>当特征值<code>&gt;1</code>说明他鼓励这种差异，反之对差异持消极态度</p> <p>a仍是学习率</p> <ul><li><p><code>Q(s,a) &lt;-- Q(s,a) + a[diff]</code></p> <p>准确的Q值</p></li> <li><p><code>wi &lt;-- wi + a[diff]f(s,a)</code></p> <p>近似的Q值</p></li></ul> <p>当权重降低，其对应的多项式变低，Q得到调整，那么更新权重成为现在的问题</p> <p>这么做的目的无非是想用相对少的数据得到一个相对好的Q函数</p> <p><strong>Optimization</strong></p> <p>最小二乘法处理特征值<code>features</code></p> <ul><li><code>Q(s,a) = w1*f1(s,a)+w2*f2(s,a)+...+wn*fn(s,a)</code></li> <li><code>Q(s,a)=w0 + w1f1(s,a)</code></li></ul> <p>Minimizing Error</p> <ul><li><code>error(w) = (1/2)*(y-Σwk*fk(x))½</code></li></ul> <p>对该函数对w求导得</p> <ul><li><code>-(y-Σwk*fk(x))fm(x)</code></li></ul> <p>Why limiting capacity can help?</p> <p>功能越多并不一定越好，这意味着更高阶的多项式，在函数曲线上更加符合</p> <p>这有可能造成过度拟合（overfitting），即为了满足一些离谱的数据，做出疯狂的拟合</p> <h4 id="policy-search"><a href="#policy-search" class="header-anchor">#</a> Policy Search</h4> <p>尝试不同的策略，看哪一个更好</p> <p><code>Q-Learning</code>：Q值接近，无法确定这是最好的行动</p> <p>让我们关注行动</p> <p>我们有一些Qvalue，向上向下调整特征值权重，看看有什么变化，好则接收，坏则丢弃，然后继续调整，就像CSP的本地搜索</p> <blockquote><p>直升飞机倒挂着飞会省四倍阻力</p> <p>ai vs. ai and train each other</p></blockquote></div></div> <!----> <div class="page-edit"><!----> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/05/30, 13:16:21</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/288d2a/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">搜索、约束满足和博弈</div></a> <a href="/pages/f0dd9d/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">不确定知识和概率推理</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/288d2a/" class="prev">搜索、约束满足和博弈</a></span> <span class="next"><a href="/pages/f0dd9d/">不确定知识和概率推理</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/c4df29/"><div>
            隐藏矢量加密和 Gray 编码优化
            <!----></div></a> <span class="date">6-1</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/5715a3/"><div>
            Android Java
            <!----></div></a> <span class="date">05-28</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/b629cf/"><div>
            Docker 和 K8s
            <!----></div></a> <span class="date">5-19</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:Arkrypto@qq.com" title="邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/Arkrypto" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="https://music.163.com/#/my/m/music/playlist?id=5123040741" title="音乐" target="_blank" class="iconfont icon-erji"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2020-2025
    <span>Arkrypto | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <!----></div><div class="global-ui"><div class="reco-bgm-panel" data-v-b1d3339e><audio id="bgm" src="/song/The Sun Also Rises.mp3" data-v-b1d3339e></audio> <div class="reco-float-box" style="bottom:10px;z-index:999999;display:none;" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><img src="/img/error.jpg" data-v-b1d3339e></div> <div class="reco-bgm-box" style="left:10px;bottom:10px;z-index:999999;" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><div class="reco-bgm-cover" style="background-image:url(/img/error.jpg);" data-v-b1d3339e><div class="mini-operation" style="display:none;" data-v-b1d3339e><i class="reco-bgm reco-bgm-pause" style="display:none;" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-play" style="display:none;" data-v-b1d3339e></i></div> <div class="falut-message" style="display:none;" data-v-b1d3339e>
          播放失败
        </div></div> <div class="reco-bgm-info" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><div class="info-box" data-v-b1d3339e><i class="reco-bgm reco-bgm-music music" data-v-b1d3339e></i>太阳照常升起</div> <div class="info-box" data-v-b1d3339e><i class="reco-bgm reco-bgm-artist" data-v-b1d3339e></i>久石譲</div> <div class="reco-bgm-progress" data-v-b1d3339e><div class="progress-bar" data-v-b1d3339e><div class="bar" data-v-b1d3339e></div></div></div> <div class="reco-bgm-operation" data-v-b1d3339e><i class="reco-bgm reco-bgm-last last" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-pause pause" style="display:none;" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-play play" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-next next" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-volume1 volume" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-mute mute" style="display:none;" data-v-b1d3339e></i> <div class="volume-bar" data-v-b1d3339e><div class="bar" data-v-b1d3339e></div></div></div></div> <div class="reco-bgm-left-box" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><i class="reco-bgm reco-bgm-left" data-v-b1d3339e></i></div></div></div></div></div>
    <script src="/assets/js/app.c56dd4ae.js" defer></script><script src="/assets/js/3.182b3e92.js" defer></script><script src="/assets/js/39.e12c41d5.js" defer></script><script src="/assets/js/171.5e1a14ad.js" defer></script>
  </body>
</html>
