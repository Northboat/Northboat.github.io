<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>马尔可夫决策和强化学习 | Arkrypto</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_3129839_xft6cqs5gc.css">
    <noscript><meta http-equiv="refresh" content="0; url=https://www.youngkbt.cn/noscript/"><style>.theme-vdoing-content { display:none }</noscript>
    <meta name="description" content="My Wiki">
    <meta name="keywords" content="Arkrypto, ComputerScience, DevOps, Crypto">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.3d49d47b.css" as="style"><link rel="preload" href="/assets/js/app.01cdd517.js" as="script"><link rel="preload" href="/assets/js/3.d2d1b22d.js" as="script"><link rel="preload" href="/assets/js/39.4614b829.js" as="script"><link rel="preload" href="/assets/js/172.1755a58c.js" as="script"><link rel="prefetch" href="/assets/js/10.b11fab32.js"><link rel="prefetch" href="/assets/js/100.05416b36.js"><link rel="prefetch" href="/assets/js/101.a069cc36.js"><link rel="prefetch" href="/assets/js/102.6233836b.js"><link rel="prefetch" href="/assets/js/103.c4f48c96.js"><link rel="prefetch" href="/assets/js/104.2d705c19.js"><link rel="prefetch" href="/assets/js/105.0f57368e.js"><link rel="prefetch" href="/assets/js/106.c66cfaa9.js"><link rel="prefetch" href="/assets/js/107.70f86da5.js"><link rel="prefetch" href="/assets/js/108.fa015535.js"><link rel="prefetch" href="/assets/js/109.995dc9e4.js"><link rel="prefetch" href="/assets/js/11.5da3d4a6.js"><link rel="prefetch" href="/assets/js/110.8f09bf6d.js"><link rel="prefetch" href="/assets/js/111.a2342491.js"><link rel="prefetch" href="/assets/js/112.05134cf0.js"><link rel="prefetch" href="/assets/js/113.582ab891.js"><link rel="prefetch" href="/assets/js/114.15cc72b2.js"><link rel="prefetch" href="/assets/js/115.adcc1d93.js"><link rel="prefetch" href="/assets/js/116.0a847ab6.js"><link rel="prefetch" href="/assets/js/117.54245b3c.js"><link rel="prefetch" href="/assets/js/118.de6ec7ed.js"><link rel="prefetch" href="/assets/js/119.f6190211.js"><link rel="prefetch" href="/assets/js/12.8a358094.js"><link rel="prefetch" href="/assets/js/120.88f7676c.js"><link rel="prefetch" href="/assets/js/121.3584f71f.js"><link rel="prefetch" href="/assets/js/122.172ae5b0.js"><link rel="prefetch" href="/assets/js/123.47760d23.js"><link rel="prefetch" href="/assets/js/124.1e7432b8.js"><link rel="prefetch" href="/assets/js/125.694ffa83.js"><link rel="prefetch" href="/assets/js/126.20565321.js"><link rel="prefetch" href="/assets/js/127.f2c1ed20.js"><link rel="prefetch" href="/assets/js/128.06101ccb.js"><link rel="prefetch" href="/assets/js/129.3ebb2fa8.js"><link rel="prefetch" href="/assets/js/13.1479b956.js"><link rel="prefetch" href="/assets/js/130.3fda3d91.js"><link rel="prefetch" href="/assets/js/131.dfc11720.js"><link rel="prefetch" href="/assets/js/132.ec0e623e.js"><link rel="prefetch" href="/assets/js/133.f1358cd1.js"><link rel="prefetch" href="/assets/js/134.9f497285.js"><link rel="prefetch" href="/assets/js/135.5ff4821f.js"><link rel="prefetch" href="/assets/js/136.7c7bc6da.js"><link rel="prefetch" href="/assets/js/137.4312f12d.js"><link rel="prefetch" href="/assets/js/138.1a8df969.js"><link rel="prefetch" href="/assets/js/139.0cf2f8a0.js"><link rel="prefetch" href="/assets/js/14.097f2d71.js"><link rel="prefetch" href="/assets/js/140.dc1d4480.js"><link rel="prefetch" href="/assets/js/141.ba54c1f0.js"><link rel="prefetch" href="/assets/js/142.8711f302.js"><link rel="prefetch" href="/assets/js/143.13d6deef.js"><link rel="prefetch" href="/assets/js/144.f0e486f2.js"><link rel="prefetch" href="/assets/js/145.59935234.js"><link rel="prefetch" href="/assets/js/146.636d73b4.js"><link rel="prefetch" href="/assets/js/147.3439e526.js"><link rel="prefetch" href="/assets/js/148.016c9481.js"><link rel="prefetch" href="/assets/js/149.63f84ebd.js"><link rel="prefetch" href="/assets/js/15.fde38a08.js"><link rel="prefetch" href="/assets/js/150.7b8cb666.js"><link rel="prefetch" href="/assets/js/151.090ed9ea.js"><link rel="prefetch" href="/assets/js/152.bd24110a.js"><link rel="prefetch" href="/assets/js/153.35312f19.js"><link rel="prefetch" href="/assets/js/154.f184f66e.js"><link rel="prefetch" href="/assets/js/155.f32f593c.js"><link rel="prefetch" href="/assets/js/156.d282fd23.js"><link rel="prefetch" href="/assets/js/157.670abf30.js"><link rel="prefetch" href="/assets/js/158.4c1d1790.js"><link rel="prefetch" href="/assets/js/159.3b93ad07.js"><link rel="prefetch" href="/assets/js/16.9268c9dc.js"><link rel="prefetch" href="/assets/js/160.aa4e1fb6.js"><link rel="prefetch" href="/assets/js/161.1063de0e.js"><link rel="prefetch" href="/assets/js/162.71afe6b5.js"><link rel="prefetch" href="/assets/js/163.6074bded.js"><link rel="prefetch" href="/assets/js/164.51abbe81.js"><link rel="prefetch" href="/assets/js/165.3cc70195.js"><link rel="prefetch" href="/assets/js/166.685535d5.js"><link rel="prefetch" href="/assets/js/167.c855e4c9.js"><link rel="prefetch" href="/assets/js/168.5692ce7d.js"><link rel="prefetch" href="/assets/js/169.d3ca34dd.js"><link rel="prefetch" href="/assets/js/17.aeb95fc1.js"><link rel="prefetch" href="/assets/js/170.1f28bf9d.js"><link rel="prefetch" href="/assets/js/171.793e78d0.js"><link rel="prefetch" href="/assets/js/173.59fa56ce.js"><link rel="prefetch" href="/assets/js/174.e259cd6f.js"><link rel="prefetch" href="/assets/js/175.3dbd319a.js"><link rel="prefetch" href="/assets/js/176.fa5fa4d2.js"><link rel="prefetch" href="/assets/js/177.cf63e897.js"><link rel="prefetch" href="/assets/js/178.e4486316.js"><link rel="prefetch" href="/assets/js/179.2326e5a7.js"><link rel="prefetch" href="/assets/js/18.47fd8bdf.js"><link rel="prefetch" href="/assets/js/180.7ae048e4.js"><link rel="prefetch" href="/assets/js/181.ed32929d.js"><link rel="prefetch" href="/assets/js/182.fe9de8f9.js"><link rel="prefetch" href="/assets/js/183.687d7231.js"><link rel="prefetch" href="/assets/js/184.daadce41.js"><link rel="prefetch" href="/assets/js/185.08d3972d.js"><link rel="prefetch" href="/assets/js/186.81e5d0dc.js"><link rel="prefetch" href="/assets/js/187.5cd34e67.js"><link rel="prefetch" href="/assets/js/188.dbd47b6e.js"><link rel="prefetch" href="/assets/js/189.3284f9ac.js"><link rel="prefetch" href="/assets/js/19.d73d6523.js"><link rel="prefetch" href="/assets/js/190.fa4c7554.js"><link rel="prefetch" href="/assets/js/191.7eec9abd.js"><link rel="prefetch" href="/assets/js/192.795a0d0b.js"><link rel="prefetch" href="/assets/js/193.1dee78d4.js"><link rel="prefetch" href="/assets/js/194.1bf4eebf.js"><link rel="prefetch" href="/assets/js/195.d40917d8.js"><link rel="prefetch" href="/assets/js/196.8477e88b.js"><link rel="prefetch" href="/assets/js/2.219b4cea.js"><link rel="prefetch" href="/assets/js/20.b4e49b87.js"><link rel="prefetch" href="/assets/js/21.dea70a0d.js"><link rel="prefetch" href="/assets/js/22.11618bd2.js"><link rel="prefetch" href="/assets/js/23.be998907.js"><link rel="prefetch" href="/assets/js/24.015abafa.js"><link rel="prefetch" href="/assets/js/25.864af69c.js"><link rel="prefetch" href="/assets/js/26.915ee4d0.js"><link rel="prefetch" href="/assets/js/27.e08520bb.js"><link rel="prefetch" href="/assets/js/28.2ed56ec3.js"><link rel="prefetch" href="/assets/js/29.23f5c86b.js"><link rel="prefetch" href="/assets/js/30.36d8f69e.js"><link rel="prefetch" href="/assets/js/31.cb5a1b95.js"><link rel="prefetch" href="/assets/js/32.7d5c2554.js"><link rel="prefetch" href="/assets/js/33.fc8e23be.js"><link rel="prefetch" href="/assets/js/34.4cb48a1c.js"><link rel="prefetch" href="/assets/js/35.fba60d86.js"><link rel="prefetch" href="/assets/js/36.51bcccd2.js"><link rel="prefetch" href="/assets/js/37.24ad7528.js"><link rel="prefetch" href="/assets/js/38.682c5fce.js"><link rel="prefetch" href="/assets/js/4.0b31af77.js"><link rel="prefetch" href="/assets/js/40.4b84a371.js"><link rel="prefetch" href="/assets/js/41.30111a99.js"><link rel="prefetch" href="/assets/js/42.4e7d02a4.js"><link rel="prefetch" href="/assets/js/43.040221cc.js"><link rel="prefetch" href="/assets/js/44.994a4407.js"><link rel="prefetch" href="/assets/js/45.b3ae36cc.js"><link rel="prefetch" href="/assets/js/46.64b30a2d.js"><link rel="prefetch" href="/assets/js/47.2e19909d.js"><link rel="prefetch" href="/assets/js/48.15e6fa18.js"><link rel="prefetch" href="/assets/js/49.edeab9a1.js"><link rel="prefetch" href="/assets/js/5.b32e1927.js"><link rel="prefetch" href="/assets/js/50.a3960fd0.js"><link rel="prefetch" href="/assets/js/51.05836a52.js"><link rel="prefetch" href="/assets/js/52.7c8ff9b7.js"><link rel="prefetch" href="/assets/js/53.7cf40edf.js"><link rel="prefetch" href="/assets/js/54.03759826.js"><link rel="prefetch" href="/assets/js/55.e7d7168e.js"><link rel="prefetch" href="/assets/js/56.7517ef56.js"><link rel="prefetch" href="/assets/js/57.20724a58.js"><link rel="prefetch" href="/assets/js/58.ba80e239.js"><link rel="prefetch" href="/assets/js/59.db100a8b.js"><link rel="prefetch" href="/assets/js/6.65459120.js"><link rel="prefetch" href="/assets/js/60.390312d8.js"><link rel="prefetch" href="/assets/js/61.386a9ad9.js"><link rel="prefetch" href="/assets/js/62.7945aa66.js"><link rel="prefetch" href="/assets/js/63.3e702b29.js"><link rel="prefetch" href="/assets/js/64.dd00441d.js"><link rel="prefetch" href="/assets/js/65.a528eb40.js"><link rel="prefetch" href="/assets/js/66.6caad9c8.js"><link rel="prefetch" href="/assets/js/67.346a9eaa.js"><link rel="prefetch" href="/assets/js/68.e4c6eca1.js"><link rel="prefetch" href="/assets/js/69.a67a8999.js"><link rel="prefetch" href="/assets/js/7.3ab074df.js"><link rel="prefetch" href="/assets/js/70.36499e2d.js"><link rel="prefetch" href="/assets/js/71.0837a2c8.js"><link rel="prefetch" href="/assets/js/72.0b008073.js"><link rel="prefetch" href="/assets/js/73.8c871bb8.js"><link rel="prefetch" href="/assets/js/74.45812c4d.js"><link rel="prefetch" href="/assets/js/75.7791010c.js"><link rel="prefetch" href="/assets/js/76.a3b0280c.js"><link rel="prefetch" href="/assets/js/77.cebd8876.js"><link rel="prefetch" href="/assets/js/78.36c75035.js"><link rel="prefetch" href="/assets/js/79.4c313d9c.js"><link rel="prefetch" href="/assets/js/8.473fc3e9.js"><link rel="prefetch" href="/assets/js/80.a3661772.js"><link rel="prefetch" href="/assets/js/81.7cd0d854.js"><link rel="prefetch" href="/assets/js/82.c4d51ca5.js"><link rel="prefetch" href="/assets/js/83.742d7133.js"><link rel="prefetch" href="/assets/js/84.ab8144b0.js"><link rel="prefetch" href="/assets/js/85.455d136b.js"><link rel="prefetch" href="/assets/js/86.ff3c9fec.js"><link rel="prefetch" href="/assets/js/87.48ce1032.js"><link rel="prefetch" href="/assets/js/88.e5ad6370.js"><link rel="prefetch" href="/assets/js/89.ab22698e.js"><link rel="prefetch" href="/assets/js/9.36000f7b.js"><link rel="prefetch" href="/assets/js/90.f8e12247.js"><link rel="prefetch" href="/assets/js/91.a34eccc6.js"><link rel="prefetch" href="/assets/js/92.7282c354.js"><link rel="prefetch" href="/assets/js/93.3d6f1df4.js"><link rel="prefetch" href="/assets/js/94.33aa0f3b.js"><link rel="prefetch" href="/assets/js/95.9e3052d6.js"><link rel="prefetch" href="/assets/js/96.ae119c41.js"><link rel="prefetch" href="/assets/js/97.a50ecd46.js"><link rel="prefetch" href="/assets/js/98.90dcb04e.js"><link rel="prefetch" href="/assets/js/99.4ba5cf1a.js">
    <link rel="stylesheet" href="/assets/css/0.styles.3d49d47b.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/logo.png" alt="Arkrypto" class="logo"> <span class="site-name can-hide">Arkrypto</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/cs/" class="nav-link">计算机科学与技术</a></div><div class="nav-item"><a href="/dev/" class="nav-link">开发与运维</a></div><div class="nav-item"><a href="/sec/" class="nav-link">网络与信息安全</a></div><div class="nav-item"><a href="/mine/" class="nav-link">我的</a></div><div class="nav-item"><a href="/archives/" class="nav-link">归档</a></div> <a href="https://github.com/Arkrypto" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/avatar.gif"> <div class="blogger-info"><h3>Arkrypto</h3> <span>就在坚冰还盖着北海的时候，我看到了怒放的梅花</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/cs/" class="nav-link">计算机科学与技术</a></div><div class="nav-item"><a href="/dev/" class="nav-link">开发与运维</a></div><div class="nav-item"><a href="/sec/" class="nav-link">网络与信息安全</a></div><div class="nav-item"><a href="/mine/" class="nav-link">我的</a></div><div class="nav-item"><a href="/archives/" class="nav-link">归档</a></div> <a href="https://github.com/Arkrypto" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>密码工程</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>数学</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>人工智能</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>人工智能导论</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/288d2a/" class="sidebar-link">搜索、约束满足和博弈</a></li><li><a href="/pages/17b001/" aria-current="page" class="active sidebar-link">马尔可夫决策和强化学习</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/17b001/#mdps" class="sidebar-link">MDPs</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/17b001/#what-is-mdps" class="sidebar-link">What is MDPs</a></li><li class="sidebar-sub-header level3"><a href="/pages/17b001/#solving-mdps" class="sidebar-link">Solving MDPs</a></li><li class="sidebar-sub-header level3"><a href="/pages/17b001/#summary" class="sidebar-link">Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/17b001/#rl" class="sidebar-link">RL</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/17b001/#model-based-learning" class="sidebar-link">Model-Based Learning</a></li><li class="sidebar-sub-header level3"><a href="/pages/17b001/#model-free-learning" class="sidebar-link">Model-Free Learning</a></li></ul></li></ul></li><li><a href="/pages/f0dd9d/" class="sidebar-link">不确定知识和概率推理</a></li><li><a href="/pages/54be26/" class="sidebar-link">隐马尔科夫和机器学习</a></li><li><a href="/pages/99e92c/" class="sidebar-link">经典人工智能算法实现</a></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>机器学习导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>深度学习框架</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06970110><div class="articleInfo" data-v-06970110><ul class="breadcrumbs" data-v-06970110><li data-v-06970110><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06970110></a></li> <li data-v-06970110><a href="/sec/#网络与信息安全" data-v-06970110>网络与信息安全</a></li><li data-v-06970110><a href="/sec/#人工智能" data-v-06970110>人工智能</a></li><li data-v-06970110><a href="/sec/#人工智能导论" data-v-06970110>人工智能导论</a></li></ul> <div class="info" data-v-06970110><div title="作者" class="author iconfont icon-touxiang" data-v-06970110><a href="https://github.com/Arkrypto" target="_blank" title="作者" class="beLink" data-v-06970110>Arkrypto</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06970110><a href="javascript:;" data-v-06970110>2022-4-24</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">马尔可夫决策和强化学习<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h2 id="mdps"><a href="#mdps" class="header-anchor">#</a> MDPs</h2> <blockquote><p>Markov Decision Processes</p> <p>马尔可夫决定过程</p></blockquote> <h3 id="what-is-mdps"><a href="#what-is-mdps" class="header-anchor">#</a> What is MDPs</h3> <p>进程：对搜索的概括</p> <p>计算可能的结果</p> <p>在<code>GridWorld</code>中，你决定向北走（因为这是最佳策略），但可能会执行失败（撞墙）</p> <p>MDP：Reward ——&gt; 结果</p> <ul><li>happy reward</li> <li>bad reward</li></ul> <p>目标很松散 ——&gt;为了最大化奖励的总和</p> <p>An MDP is defined by</p> <ul><li><p>a set of states s</p></li> <li><p>a set of actions a</p></li> <li><p>a transition function T(s, a, s')</p> <ul><li><p>Probability that a from s leads to s', called P(s'| s, a)</p> <p>在s状态执行a行为到达s'的代价</p></li> <li><p>Also called the model or the dynamics</p> <p>不同于搜索，这个后继函数有很多个，如在每个地点都可以向东南西北移动</p></li></ul></li> <li><p>a reward function R(s, a, s')</p> <ul><li><p>sometimes just R(s) or R(s')</p> <p>奖惩制度，有时只取决起点或终点</p></li></ul></li> <li><p>a start state</p></li> <li><p>maybe a terminal state</p></li></ul> <p>MDPs是不确定性搜索问题</p> <ul><li>强化学习的基础</li></ul> <p>expectimax（最大期望算法）算法可以MDP问题</p> <p>action outcomes depend on</p> <ul><li>未来要到达的状态</li> <li>你要执行的行动</li></ul> <p>MDPs适合嘈杂的世界</p> <p><strong>Grid World</strong></p> <p>Policy：策略</p> <ul><li>通过状态告诉你动作的功能</li> <li>如在地图上每个点标好你该往哪个方向走</li></ul> <p>optimal policy：最优策略</p> <ul><li>或许存在很多等效策略</li></ul> <p>competition</p> <ul><li>移动奖励（负0.1）是那么微不足道而不值得冒险去坑附近</li> <li>宁愿什么都不做，也不愿犯错</li></ul> <p>当移动代价变得更大，策略将更倾向与冒险在坑附近</p> <p>当更更大时，甚至有可能直接跳坑而避免移动花销</p> <p><strong>Racing</strong></p> <p>states：</p> <ul><li>cool</li> <li>warm</li> <li>overheated：risk the danger of breaking</li></ul> <p>跟据当前的温度决定是加速还是减速</p> <p><strong>Racing Search Tree</strong></p> <blockquote><p>tool：epectimax search</p></blockquote> <p>actions：</p> <ul><li>slower</li> <li>faster</li></ul> <p>state：</p> <ul><li>warm</li> <li>cool</li> <li>over heated</li></ul> <p>这棵树是无限的</p> <ul><li>Q state：选择了但还没行动的过度状态</li></ul> <p><strong>Utilities of Sequences</strong></p> <p>实用程序的选择顺序</p> <ul><li>more or less</li> <li>now or later</li></ul> <p>隐含的权衡</p> <p><strong>discount</strong></p> <p>对奖励的贬值，对晚来的价值施以惩罚，如每走一步，未得到的价值便腐朽0.8，0.8便是折扣</p> <p>当折扣越大，即<code>λ</code>越小，agent将变得越贪婪，越在意眼前的价值，而不是以后获得更大的利益</p> <p><strong>Preferences</strong></p> <p>假设偏好是固定的</p> <p>two ways to define utilities</p> <ul><li>additive utility</li> <li>discounted utility</li></ul> <p>如何处理无限的问题</p> <ul><li>Finite horizen：similar to depth-limited search，即限定树的深度</li> <li>Discounting：价值总是贬值，将无限接近于0</li> <li>Absorbing state：使用一系列终止状态，即</li></ul> <p>Markov decision processes：</p> <ul><li>状态集</li> <li>初始状态</li> <li>行为集</li> <li>过渡函数：提供的是概率</li> <li>奖惩机制</li></ul> <p>它的输出是每个state上对应的action，他实际上并没有真正在试错，而是去给每个状态分配最佳的行动，这就是MDP</p> <h3 id="solving-mdps"><a href="#solving-mdps" class="header-anchor">#</a> Solving MDPs</h3> <p>Quantities：</p> <ul><li>Policy：map of states of actions</li> <li>Utility：sum of discounted reward</li> <li>Values：expected future utility from a state（max node）</li> <li>Q Values：expected future from a q-state（chance node）</li></ul> <p>Optimal Quantities</p> <ul><li>V(s)*：状态的期望值（或许是平均值）</li> <li>Q*(s, a)：在状态s执行动作a后起的最佳作用</li> <li>P*(s)：当前状态的最佳策略（算法产出）</li></ul> <p>expectimax search可以解决这一问题，估算价值，选出最大价值，赋值</p> <p>考虑一下其他的算法</p> <ul><li><p><code>V*(s) = maxQ*(s, a)</code></p> <p>虽然Q*(s, a)还不知道怎么算</p></li> <li><p><code>Q*(s, a) = avg(sum(R(s, a, s') + λV*(s'))</code></p> <p>这是一个递归的定义，因为你并不知道V*(s')直到搜索到终点</p></li></ul> <p>此之谓贝尔曼方程：Bellman Equations</p> <ul><li>take correct first action</li> <li>kepp being optimal</li></ul> <p>回顾一下Racing Search Tree</p> <p>他是无限的，并且只有三种状态，如果用expectimax search，将会有指数级的重复工作（子树）</p> <h4 id="value-iteration"><a href="#value-iteration" class="header-anchor">#</a> Value Iteration</h4> <p>价值迭代算法</p> <ul><li>from the bottom（deep enough）, recur the top</li> <li><code>V*(s) = maxΣT(s,a,s')[R(s,a,s') + λV*(s')]</code></li></ul> <p>利用贝尔曼方程确实可以搜索到底部并且递归回顶部，在这个递归过程中，各节点的值是不断更新的，且更加准确，直到保持稳定，即递归完毕</p> <ul><li>这个收敛的过程称作<code>bellman update</code></li></ul> <p><strong>Computing Time-Limited Values</strong></p> <p>对于一颗无限树，采用时间限制其递归深度，令V*(s)尽可能准确</p> <p>因为条件有限，我们无法完整进行贝尔曼算法，即只能尽可能的接近V*(s)</p> <ul><li><code>Vk(s) = avg(sum(R(s, a, s') + λVk(s')))</code></li></ul> <p>其中<code>Vk(s)、Vk(s')</code>都取其均值</p> <ul><li>take average</li> <li>像一个单层的expectimax搜索，但不同的是，他会由于递归深度的增加不断调整Vk值</li></ul> <p><strong>Convergence</strong></p> <p>VK compute</p> <p>一个k层树和一个k+1层树</p> <p>由于搜索深度增加，对于未来某节点的折扣也增加，也就是说越往后对总值的影响应是越小，细微调整</p> <p>当discount&gt;=1，没有趋同保证</p> <h4 id="policy-evaluation"><a href="#policy-evaluation" class="header-anchor">#</a> Policy Evaluation</h4> <p>策略评估方法</p> <h5 id="fixed-policies"><a href="#fixed-policies" class="header-anchor">#</a> fixed Policies</h5> <p>固定的策略</p> <ul><li>do the optimal action</li> <li>do what Pi says</li> <li>easier than the optimal</li></ul> <p>假设你的固定策略选出的后继节点是最佳的</p> <ul><li><code>VΠ(s) = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></li></ul> <p>固定策略例如：一直向右走；一直向前走</p> <p>列举所有策略，评估所有策略，选择得分最高的策略</p> <h5 id="policy-evaluation-2"><a href="#policy-evaluation-2" class="header-anchor">#</a> policy evaluation</h5> <p>输入一个策略，执行策略，得到该策略的值向量</p> <p><code>VΠ = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></p> <h5 id="policy-extraction"><a href="#policy-extraction" class="header-anchor">#</a> Policy Extraction</h5> <p>即使当找到了相邻的最佳值，仍然要做一次expectimax去找到导致这个最佳值的行动</p> <ul><li>从值中找出行动，以更新策略</li></ul> <p>价值驱动决策</p> <p>Computing Actions from Q-Values</p> <h4 id="policy-iteration"><a href="#policy-iteration" class="header-anchor">#</a> Policy Iteration</h4> <p>价值迭代的问题</p> <ul><li>每次迭代将会耗费<code>O(s^2*A)</code>，这很慢</li> <li>每个状态的最大值很少改变，这意味着做了很多低效工作</li></ul> <p>正确策略下的无用选择 ——&gt; 错误的策略试错</p> <p>我们采用策略迭代</p> <ul><li>首先选择一些策略，并执行他们，估算状态价值</li> <li>改善你的策略，再次考虑之前的行动，重复估值</li> <li>直到策略收敛</li></ul> <p>可以证明它是最佳且收敛的，并且在很多情况比价值迭代收敛得更快</p> <p>VΠ是由当前策略得到的当前“最佳值”</p> <p><code>VΠ = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></p> <p>根据这个当前最佳值，更新上一步的策略，比如我上一步策略原来是往北走，但这个最佳值得往东走，那么我更新上一步的策略为向东走</p> <ul><li>MDPs本质上便是找到每步的最佳策略，值迭代同时考虑策略和价值，在每步做出最佳选择；策略迭代通过值去找到更优的策略</li> <li>二者都是迭代，从叶子回溯到顶部</li></ul> <p>通常根据最后值的变化来确定是否已经收敛</p> <h3 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h3> <ul><li>compute optimal values：both can</li> <li>compute values for partivular policy：policy evaluation（策略评估）</li> <li>turn your values into policy：use policy extraction（策略抽取）</li></ul> <p>通常Policy Iteration是policy evaluation和policy improvement交替执行直到收敛</p> <p>Value Iteration是寻找Optimal value function和执行一次policy extraction</p> <ul><li>均属于动态规划算法</li></ul> <p><strong>Double-Bandit MDP</strong></p> <p>两台老虎机，一台（blue）拉一次给一块钱；另一台（red）拉一次给0元或2元。共拉一百次</p> <p>更优的策略？</p> <ul><li><code>red one</code>获得2元的概率为0.75</li></ul> <p>平均上</p> <ul><li>blue：100元</li> <li>red：150元</li></ul> <p>当获得2元的概率未知，尝试red one去获得信息</p> <p>core of reinfocement Learning：exploraton</p> <p>只能探索才能获取更多信息</p> <p>pay for the infomation and get return</p> <p>甚至不需要MDP算法，只需要不断探索和基本的数学直觉，试出概率</p> <h2 id="rl"><a href="#rl" class="header-anchor">#</a> RL</h2> <blockquote><p>Reinforcement learning：强化学习</p> <p>It's about how to learn behaviors</p></blockquote> <ul><li>Agent —actions—&gt; Environment</li> <li>Environment —state/reward—&gt; Agent</li></ul> <p>Agent和Environment都是动态变化的</p> <p>Basic idea：</p> <ul><li>agent接收奖惩反馈</li> <li>奖惩函数决定agent的效用</li> <li>为了最大化奖励，必须去学习最优行动</li> <li>所有的学习基于观察样例后的结果</li></ul> <p>learning rather than plan</p> <p>Examples：</p> <ul><li>Robot dog learning to Walk</li> <li>Snake rebot sidewingding（爬墙）</li></ul> <p>因为真实世界的规则并不是确定的，难以建模，这时让程序根据概率学习正确的行为显得更加高效</p> <ul><li><p>Toddler Robot（幼儿机器人）</p> <p>know how to stand after fall down</p></li></ul> <p>机器学习的最开始，他是不知道怎么做的，只是来回摆动，因为他不知道怎么获取奖励，于是开始瞎几把试，当偶然获取奖励后，他将根据奖惩制度完善自己的行动策略，从而行动得更加高效</p> <p>Still assume a Markov decision process</p> <ul><li><p>a set of states</p></li> <li><p>a set of actions</p></li> <li><p>a model T(s,a,s')</p> <p>原为 a successor function T(s,a,s')</p></li> <li><p>a reward function R(s,a,s')</p></li></ul> <p>Still looking for a policy</p> <p>The defference：We don't know T or R</p> <ul><li><p>不知道哪个状态是好的或哪个动作是好的</p> <p>就像那个老虎机不知道掉落概率</p></li> <li><p>必须真正去行动和访问状态去学习，去获取必要信息</p></li></ul> <p>Offline（MDPs） vs. Online（RL）</p> <ul><li>Offline Solution</li> <li>Online Learning</li></ul> <h3 id="model-based-learning"><a href="#model-based-learning" class="header-anchor">#</a> Model-Based Learning</h3> <p>Basic idea：</p> <ul><li>learn an approximate model based on experiences</li> <li>solve for values as if the learned model were correct</li></ul> <p>现根据经验构建模型，再使用问题求解方法去计算当前模型</p> <p>就像一个CSP问题我们不知道联系，得先建立相邻状态联系</p> <p>step1：learn empirical MDP model</p> <ul><li>为每个状态和动作做产出（outcomes）统计</li> <li>常态化评估函数T(s,a,s')</li> <li>每当经历<code>s—a—&gt;s'</code>时计算回报函数R(s,a,s')</li></ul> <p>step2：solve the learned MDP（近似的MDP问题）</p> <ul><li>use value iteration</li> <li>use policy iteration</li> <li>......</li></ul> <p>T和R是未知的，但状态空间和行为空间被分配了，要做的就是收集更多数据，动态改善你的模型，估计T和R函数</p> <p>where the reward function come from</p> <ul><li>depend on the human designer</li></ul> <p>how to calculate T function</p> <ul><li>in a simple example, may just looking at the frequencies（频率）</li></ul> <p>计算概率权值：E（概率x值）</p> <ul><li><p>Known P(A)：E(A) = ΣP(a)*a</p></li> <li><p>Unknown P(A)</p> <ul><li><p>Model Based：E(A) = avg(sum(P(a)*a))</p> <p>以某种策略重新计算概率</p></li> <li><p>Model free：E(A) = (1/N)*sum(a)</p> <p>我们认为各种可能概率是相等的，因为尚未总结出规律</p></li> <li><p>二者区别在于是否按概率加权计算均值</p></li></ul></li></ul> <h3 id="model-free-learning"><a href="#model-free-learning" class="header-anchor">#</a> Model-Free Learning</h3> <h4 id="value-learning"><a href="#value-learning" class="header-anchor">#</a> Value Learning</h4> <blockquote><p>Passive Reinforcement Learning</p> <p>我们不担心如何在世界模型中行动，只是观察行动并视图估计此代理的状态值</p></blockquote> <p>Simplified task：policy evaluation</p> <ul><li>input：a fixed policy（遵循某一策略）</li> <li>don't know T(s,a,s')</li> <li>don't know R(s,a,s')</li> <li>goal：learn the state values</li></ul> <p>Direct &amp; Indirect Evaluation</p> <blockquote><p>直接估值和间接估值</p></blockquote> <p>直接估值平均观察到的样本值，直接问这一步会有多少<code>reward</code>，仅仅依据实验出的结果的各状态值</p> <p>如直接对于个节点的可能取值求均值作为其状态值，如对C节点使用四次策略</p> <ul><li><p>C向D -1，D退出+10</p> <p>C向D -1，D退出+10</p> <p>C向D -1，D退出+10</p> <p>C向A -1，A退出-10</p> <p>那么取均值则为<code>(9+9+9-11)/4=4</code></p></li></ul> <p>不需要对T/R做任何事，求均值就行了，只关注值；这不能达到超精确，但随着数据增加总会愈加接近</p> <p>要做的事很明确：</p> <ul><li>选择一个节点</li> <li>多次使用策略进行扩展</li> <li>对扩展结果进行分析取均</li> <li>对该节点赋值得到<code>V(s)</code></li> <li>更新值和策略</li></ul> <p>这一过程始终没用到T/R函数</p> <ul><li><code>VΠ(s) &lt;-- (1/n)Σsample(i)</code></li></ul> <p>注意这里所有的<code>V(s')</code>都应乘上一个<code>λ(&lt;=1)</code>作为时间惩罚（贬值）</p> <p>Temporal difference learning：</p> <ul><li><code>sample = R(s,Π(s), s') + λV(s')</code></li> <li><code>VΠ(s) &lt;-- (1-a)VΠ(s) + (a)sample</code></li></ul> <p>以上为更新已走过节点的方法</p> <p>每次获得新的sample，都对刚走过的状态<code>s</code>进行更新，以接近精确值</p> <p>在这一过程中，我们从未建立世界模型，即T/R函数，只是根据样例值不断更新状态值，随着时间的推移，将得到精确值</p> <p>优化求均值的方法，让越接近的经历比以前的经历更重要，因为我们后来计算的结果总是更加准确</p> <ul><li><p><code>xn = (xn + (1-a)*xn-1 + (1-a)^2*xn-2+...) / 1+(1-a)+(1-a)^2+...</code></p> <p>xn为第n个样例</p></li> <li><p>这里的a为学习率，应用于迭代方程中</p></li></ul> <p>由于我们从未构建模型，也没有T/R函数，根本无从进行策略迭代</p> <p>为什么不学习<code>Q-Value</code>而是<code>V-Value</code>？</p> <p>没有理由，他不仅同样能实现更新Value，而且可以用于策略更新，属于积极的学习</p> <h4 id="q-learning"><a href="#q-learning" class="header-anchor">#</a> Q-Learning</h4> <blockquote><p>Active Reinforcement Learning</p> <p>担心数据从何处收集，担心采取行动</p></blockquote> <p>also</p> <ul><li>don't know the transitions T</li> <li>don't know the reward R</li> <li>choose the actions now（当前做的）</li> <li>goal：learn the optimal policy/values</li></ul> <p>不同于MDPs，这不是离线测试（毕竟不知道T/R，无法进行推测），而是真切地采取行动</p> <p>iteration</p> <ul><li>从一个确定状态值开始</li> <li>计算该状态值下一层每个状态的Q-Value和Value</li> <li>通过下一层的Q-Value/Value更新该层的Q-Value/Value</li> <li>迭代这一过程，更新所有Q-Value/Value</li></ul> <p>Value Iteration</p> <ul><li><code>Vk+1(s) &lt;-- maxΣT(s,a,s')[R(s,a,s') + λVk(s')]</code></li></ul> <p>Q-Value Iteration</p> <ul><li><code>Qk+1(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxQk(s',a')]</code></li></ul> <p>在这里使用的样例和更新策略</p> <ul><li><p><code>sample = R(s,a,s') + λmaxQk(s',a')</code></p></li> <li><p><code>Q(s,a) &lt;-- (1-a)Q(s,a) + (a)sample</code></p> <p>这个常量a称为学习率</p></li></ul> <p>举例：crawler bot（爬虫机器人）</p> <p>Q-Learning is called off-policy learning</p> <p>Caveats（警告）</p> <ul><li>have to explore enough</li> <li>have to eventually make the learning rate small enough（收敛）</li> <li>...but not decrease it too quickly</li> <li>it doesn't matter how you select actions</li></ul> <table><thead><tr><th>Problem</th> <th>Goal</th> <th>Technique</th></tr></thead> <tbody><tr><td>Known MDP</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>Value/Policy iteration; Policy evaluation</td></tr> <tr><td>Unknown MDP: Model-Based</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>VI/PI on approximate MDP; PE on approximate MDP</td></tr> <tr><td>Unknown MDP: Model-Free</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>Q-learning; Value learning</td></tr></tbody></table> <p>均使用贝尔曼方程进行递归计算</p> <p>Exploration（探索）vs. exploitation（开发）</p> <p><strong>Epsilon Greedy</strong></p> <p>Exploration function</p> <ul><li><p>探索未知节点，收集更多经验：random actions（ε epsilon-greedy）</p> <p>当ε越大，随机度越高，当为0，策略确定</p></li> <li><p>探索方程将根据一个节点的“经验”，如访问过多少次，来给予相应的奖励（访问越多，奖励越低）</p></li> <li><p><code>f(u,n) = u + k/n</code>（基数+奖励/访问次数）</p></li> <li><p>这样能有效腐烂一些无用的节点（越多访问奖励越少）</p></li></ul> <p>Q-Update：加入探索方程</p> <ul><li><p>Regular Q-Update:</p> <p><code>Q(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxQ(s',a')]</code></p></li> <li><p>Modified Q-Update:</p> <p><code>Q(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxf(Q(s',a'),N(s',a'))]</code></p></li></ul> <p>Regret</p> <h4 id="approximate-q-learning"><a href="#approximate-q-learning" class="header-anchor">#</a> Approximate Q-Learning</h4> <p>在实际问题中，状态数、动作会很多很多，很难在Q-Table中去储存每一个Q-Value，这个时候只能做估计</p> <p>w为权重，f为特征值（features）</p> <ul><li><code>V(s) = w1*f1(s)+w2*f2(s)+...+wn*fn(s)</code></li> <li><code>Q(s,a) = w1*f1(s,a)+w2*f2(s,a)+...+wn*fn(s,a)</code></li></ul> <p>你的Q值将是很多经验的加权和，如f1为跳楼的特征值，f2为纵火的特征值，Q将这些情况的经验汇总以某些权重组合</p> <p>当特征值<code>&gt;1</code>说明他鼓励这种差异，反之对差异持消极态度</p> <p>a仍是学习率</p> <ul><li><p><code>Q(s,a) &lt;-- Q(s,a) + a[diff]</code></p> <p>准确的Q值</p></li> <li><p><code>wi &lt;-- wi + a[diff]f(s,a)</code></p> <p>近似的Q值</p></li></ul> <p>当权重降低，其对应的多项式变低，Q得到调整，那么更新权重成为现在的问题</p> <p>这么做的目的无非是想用相对少的数据得到一个相对好的Q函数</p> <p><strong>Optimization</strong></p> <p>最小二乘法处理特征值<code>features</code></p> <ul><li><code>Q(s,a) = w1*f1(s,a)+w2*f2(s,a)+...+wn*fn(s,a)</code></li> <li><code>Q(s,a)=w0 + w1f1(s,a)</code></li></ul> <p>Minimizing Error</p> <ul><li><code>error(w) = (1/2)*(y-Σwk*fk(x))½</code></li></ul> <p>对该函数对w求导得</p> <ul><li><code>-(y-Σwk*fk(x))fm(x)</code></li></ul> <p>Why limiting capacity can help?</p> <p>功能越多并不一定越好，这意味着更高阶的多项式，在函数曲线上更加符合</p> <p>这有可能造成过度拟合（overfitting），即为了满足一些离谱的数据，做出疯狂的拟合</p> <h4 id="policy-search"><a href="#policy-search" class="header-anchor">#</a> Policy Search</h4> <p>尝试不同的策略，看哪一个更好</p> <p><code>Q-Learning</code>：Q值接近，无法确定这是最好的行动</p> <p>让我们关注行动</p> <p>我们有一些Qvalue，向上向下调整特征值权重，看看有什么变化，好则接收，坏则丢弃，然后继续调整，就像CSP的本地搜索</p> <blockquote><p>直升飞机倒挂着飞会省四倍阻力</p> <p>ai vs. ai and train each other</p></blockquote></div></div> <!----> <div class="page-edit"><!----> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/05/30, 13:16:21</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/288d2a/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">搜索、约束满足和博弈</div></a> <a href="/pages/f0dd9d/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">不确定知识和概率推理</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/288d2a/" class="prev">搜索、约束满足和博弈</a></span> <span class="next"><a href="/pages/f0dd9d/">不确定知识和概率推理</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/2ce9d5/"><div>
            可擦除签名
            <!----></div></a> <span class="date">6-5</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/c4df29/"><div>
            隐藏矢量加密和 Gray 编码优化
            <!----></div></a> <span class="date">6-1</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/b629cf/"><div>
            Docker 和 K8s
            <!----></div></a> <span class="date">5-19</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:Arkrypto@qq.com" title="邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/Arkrypto" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="https://music.163.com/#/my/m/music/playlist?id=5123040741" title="音乐" target="_blank" class="iconfont icon-erji"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2020-2025
    <span>Arkrypto | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <!----></div><div class="global-ui"><div class="reco-bgm-panel" data-v-b1d3339e><audio id="bgm" src="/song/The Sun Also Rises.mp3" data-v-b1d3339e></audio> <div class="reco-float-box" style="bottom:10px;z-index:999999;display:none;" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><img src="/img/error.jpg" data-v-b1d3339e></div> <div class="reco-bgm-box" style="left:10px;bottom:10px;z-index:999999;" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><div class="reco-bgm-cover" style="background-image:url(/img/error.jpg);" data-v-b1d3339e><div class="mini-operation" style="display:none;" data-v-b1d3339e><i class="reco-bgm reco-bgm-pause" style="display:none;" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-play" style="display:none;" data-v-b1d3339e></i></div> <div class="falut-message" style="display:none;" data-v-b1d3339e>
          播放失败
        </div></div> <div class="reco-bgm-info" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><div class="info-box" data-v-b1d3339e><i class="reco-bgm reco-bgm-music music" data-v-b1d3339e></i>太阳照常升起</div> <div class="info-box" data-v-b1d3339e><i class="reco-bgm reco-bgm-artist" data-v-b1d3339e></i>久石譲</div> <div class="reco-bgm-progress" data-v-b1d3339e><div class="progress-bar" data-v-b1d3339e><div class="bar" data-v-b1d3339e></div></div></div> <div class="reco-bgm-operation" data-v-b1d3339e><i class="reco-bgm reco-bgm-last last" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-pause pause" style="display:none;" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-play play" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-next next" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-volume1 volume" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-mute mute" style="display:none;" data-v-b1d3339e></i> <div class="volume-bar" data-v-b1d3339e><div class="bar" data-v-b1d3339e></div></div></div></div> <div class="reco-bgm-left-box" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><i class="reco-bgm reco-bgm-left" data-v-b1d3339e></i></div></div></div></div></div>
    <script src="/assets/js/app.01cdd517.js" defer></script><script src="/assets/js/3.d2d1b22d.js" defer></script><script src="/assets/js/39.4614b829.js" defer></script><script src="/assets/js/172.1755a58c.js" defer></script>
  </body>
</html>
